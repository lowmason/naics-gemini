# Base training configuration for the NAICS SADC pipeline

# Experiment identification
experiment_name: sadc_default
seed: 42

# Curriculum scheduler configuration (Structure-Aware Dynamic Curriculum)
curriculum:
  phase1_end: 0.3  # Structural initialization cutoff (fraction of epochs)
  phase2_end: 0.7  # Geometric refinement cutoff (fraction of epochs)
  phase3_end: 1.0  # False negative mitigation cutoff (fraction of epochs)
  tree_distance_alpha: 1.5  # Inverse-distance weighting exponent for negatives
  sibling_distance_threshold: 2.0  # Distance threshold for sibling masking
  fn_curriculum_start_epoch: 10  # Epoch to begin false-negative elimination
  fn_cluster_every_n_epochs: 5  # How often to refresh clustering during Phase 3
  fn_num_clusters: 500  # Number of clusters used for false-negative elimination

# Paths
dirs:
  data_dir: ./data
  log_dir: ./logs
  output_dir: ./outputs
  checkpoint_dir: ./checkpoints
  conf_dir: ./conf
  docs_dir: ./docs

# Data loading configuration
data_loader:
  batch_size: 12
  num_workers: 4
  val_split: 0.05

  tokenization:
    tokenizer_name: sentence-transformers/all-MiniLM-L6-v2
    max_length: 512
    padding: max_length
    truncation: true

  streaming:
    descriptions_parquet: ./data/naics_descriptions.parquet
    triplets_parquet: ./data/naics_training_pairs
    distances_parquet: ./data/naics_distances.parquet
    relation_matrix_parquet: ./data/naics_relation_matrix.parquet
    distance_matrix_parquet: ./data/naics_distance_matrix.parquet
    relations_parquet: ./data/naics_relations.parquet
    tokenizer_name: sentence-transformers/all-MiniLM-L6-v2
    max_length: 512
    seed: 42

# Model configuration
model:
  base_model_name: sentence-transformers/all-MiniLM-L6-v2

  lora:
    r: 8
    alpha: 16
    dropout: 0.1

  moe:
    num_experts: 4
    top_k: 2
    hidden_dim: 768
    load_balancing_coef: 0.01

  eval_sample_size: 300
  eval_every_n_epochs: 1

# Loss configuration
loss:
  temperature: 0.07
  curvature: 1.0
  base_margin: 0.5
  hierarchy_weight: 0.325  # Weight for hierarchy preservation loss (increased from 0.1)
  rank_order_weight: 0.275  # Weight for rank order preservation loss (Spearman correlation)
  radius_reg_weight: 0.01  # Weight for radius regularization to prevent instability

# Training configuration
training:
  learning_rate: 1e-4  # Reduced from 2e-4 to prevent radius instability
  weight_decay: 0.01
  warmup_steps: 500
  use_warmup_cosine: false  # Set to true for large training jobs with many epochs

  trainer:
    max_epochs: 20
    accelerator: auto
    devices: 1
    precision: "16-mixed"
    gradient_clip_val: 1.0
    accumulate_grad_batches: 2
    log_every_n_steps: 10
    val_check_interval: 1.0
