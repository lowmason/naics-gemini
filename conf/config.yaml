defaults:
  - _self_
  - curriculum: default

# Experiment identification
experiment_name: ${curriculum.name}
seed: 42

# Paths
dirs:
  data_dir: ./data
  log_dir: ./logs
  output_dir: ./outputs
  checkpoint_dir: ./checkpoints
  conf_dir: ./conf
  docs_dir: ./docs

# Data loading configuration
data_loader:
  batch_size: 12
  num_workers: 4
  val_split: 0.05
  
  tokenization:
    tokenizer_name: sentence-transformers/all-MiniLM-L6-v2
    max_length: 512
    padding: max_length
    truncation: true
  
  streaming:
    descriptions_parquet: ./data/naics_descriptions.parquet
    triplets_parquet: ./data/naics_training_pairs
    distances_parquet: ./data/naics_distances.parquet
    distance_matrix_parquet: ./data/naics_distance_matrix.parquet
    relations_parquet: ./data/naics_relations.parquet
    tokenizer_name: sentence-transformers/all-MiniLM-L6-v2
    max_length: 512
    seed: 42

# Model configuration
model:
  base_model_name: sentence-transformers/all-MiniLM-L6-v2
  
  lora:
    r: 8
    alpha: 16
    dropout: 0.1
  
  moe:
    num_experts: 4
    top_k: 2
    hidden_dim: 1024
    load_balancing_coef: 0.01
  
  eval_sample_size: 500  # Increased for more stable hierarchy metrics
  eval_every_n_epochs: 1  # Evaluate every epoch for better monitoring

# Loss configuration
loss:
  temperature: 0.07
  curvature: 1.0
  hierarchy_weight: 0.1  # Weight for hierarchy preservation loss (0.0 to disable)

# Training configuration
training:
  learning_rate: 2e-4
  weight_decay: 0.01
  warmup_steps: 500
  
  trainer:
    max_epochs: 15  # Reduced to prevent unnecessary training
    accelerator: auto
    devices: 1
    precision: "16-mixed"
    gradient_clip_val: 1.0
    accumulate_grad_batches: 2
    log_every_n_steps: 10
    val_check_interval: 1.0
